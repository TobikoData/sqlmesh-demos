gateways:
    # use this for a realistic project
    databricks_demo:
        connection:
            concurrent_tasks: 24
            type: databricks
            server_hostname: dbc-b9f590c4-0a08.cloud.databricks.com
            http_path: /sql/protocolv1/o/5705746990502068/0603-211256-ns7ii2e2
            access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}
            catalog: catalogtest
        state_connection:
            type: postgres
            host: {{ env_var('SQLMESH_STATE_HOST') }}
            port: 5432
            user: {{ env_var('SQLMESH_STATE_USERNAME') }}
            password: {{ env_var('SQLMESH_STATE_PASSWORD') }}
            database: sqlmesh_state_databricks_demo
        test_connection:
              type: spark
              config:
                # Run Spark locally with one worker thread
                "spark.master": "local"
                # Move data under /tmp so that it is only temporarily persisted
                "spark.sql.warehouse.dir": "/tmp/data_dir"
                "spark.driver.extraJavaOptions": "-Dderby.system.home=/tmp/derby_dir"

default_gateway: databricks_demo

model_defaults:
    dialect: spark

# enables synchronized deployments to prod when a PR is merged
cicd_bot:
    type: github
    merge_method: squash
    enable_deploy_command: true
    auto_categorize_changes:
      external: full
      python: full
      sql: full
      seed: full

plan:
  enable_preview: true

# list of users that are allowed to approve PRs for synchronized deployments
users:
- username: sung_sqlmesh_demo
  github_username: sungchun12
  roles:
    - required_approver
- username: afzal_sqlmesh_demo
  github_username: afzaljasani
  roles:
    - required_approver